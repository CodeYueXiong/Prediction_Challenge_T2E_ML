# Iterate over all features to find the best splitting variable and split point
#' @param y set of values in the node
#' @param features a matrix of features with the same
#' number of rows as the length of set_of_values
#' @return
find_best_split <- function(y, features)
{
index_columns <- 1:NCOL(features)
overall_minimal_entropy <- Inf
best_cutoff <- NA
best_feature <- NA
cat("Searching through feature...\n")
# create a progress bar
pb <- txtProgressBar(min = 0, max = NCOL(features), style = 3)
# update if label j has the smaller entropy
for(j in index_columns)
{
entropy_and_cutoff_j <- find_split_point(y, features[,j,drop=T])
if(entropy_and_cutoff_j$minimal_entropy < overall_minimal_entropy)
{
overall_minimal_entropy <- entropy_and_cutoff_j$minimal_entropy
best_cutoff <- entropy_and_cutoff_j$best_cutoff
best_feature <- j
}
setTxtProgressBar(pb, j)
}
close(pb)
return(list(minimal_entropy = overall_minimal_entropy,
best_cutoff = best_cutoff,
best_feature = best_feature))
}
entropy_given_number_obs <- function(n_k, n_not_k)
{
n = n_k + n_not_k
pi_k = n_k / n
pi_not_k = n_not_k / n
if(n_k == 0 | n_not_k == 0) return(0) # zero impurity
# compute for entropy function I(N) = -sum(pi(k)*log(pi(k)))
entropy_fun <- function(pi) - pi * log(pi)
entropy <- entropy_fun(pi_k) + entropy_fun(pi_not_k)
return(entropy)
}
#' Function to get entropy for a node set
#'
#' @param y set of values in this node
#' @return The entropy of the set
#'
entropy_node <- function(y)
{
n <- length(y)
classes <- unique(y) # unique labels in the set
I_N <- 0 # the entropy of the set
for(k in classes){
n_Nk <- sum(y == k)
# the entropy for this class
e <- mean(y == k) * entropy_given_number_obs(n_Nk, n - n_Nk)
# update I_N
I_N <- I_N + e
}
return(list(entropy = I_N,
number_values = n))
}
#' Calculate entropy based on a cut point
#'
#' @param indicator vector of logical values (see details)
#' @param y set of outcome values
#' @return entropy
#'
#' @details Function that calculates the entropy based on a set of logical values
#' that indicate to which class (of the two groups in the hypothesis space)
#' the observation belongs to
entropy_overall <- function(indicator, y)
{
n <- length(y)
entropy_left <- entropy_node(y[indicator])
entropy_right <- entropy_node(y[!indicator])
overall_entropy <-
entropy_left$number_values / n * entropy_left$entropy +
entropy_right$number_values / n * entropy_right$entropy
return(overall_entropy)
}
stump <- find_best_split(y_train, x_train)
library(tibble)
# reshape
dim(x_train) <- c(nrow(x_train), 784)
dim(x_test) <- c(nrow(x_test), 784)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
# convert to data.frame
x_train <- as_tibble(as.data.frame(x_train))
x_test <- as_tibble(as.data.frame(x_test))
#' Function to find the best split point for *one* feature
#' @param y set of outcome values in the node
#' @param features a matrix of features with the same number of rows as the length of set_of_values
#' @return a list with minimal entropy and best cut off value #’
find_split_point <- function(y, feature)
{
n <- length(y)
minimal_entropy <- Inf # some large value
best_cutoff <- NA
set_unique_values_x <- sort(unique(feature))
for(t in set_unique_values_x){
is_smaller_than_t <- feature < t
# calculate entropy based on the indicator function
# and the outcome y
entropy_for_this_t <- entropy_overall(indicator = is_smaller_than_t, y)
if(entropy_for_this_t <= minimal_entropy)
{
minimal_entropy <- entropy_for_this_t
best_cutoff <- t
}
}
return(list(minimal_entropy = minimal_entropy,
best_cutoff = best_cutoff))
}
# entropy_overall <- function(indicator, y)
# {
#
#   n <- length(y)
#   entropy_left <- entropy_node(y[indicator])
#   entropy_right <- entropy_node(y[!indicator])
#   overall_entropy <-
#     entropy_left$number_values / n * entropy_left$entropy +  ## I(N1)*n(N1)/n + I(N2)*n(N2)/n
#     entropy_right$number_values / n * entropy_right$entropy
#
#   return(overall_entropy)
#
# }
# Iterate over all features to find the best splitting variable and split point
#' @param y set of values in the node
#' @param features a matrix of features with the same
#' number of rows as the length of set_of_values
#' @return
find_best_split <- function(y, features)
{
index_columns <- 1:NCOL(features)
overall_minimal_entropy <- Inf
best_cutoff <- NA
best_feature <- NA
cat("Searching through feature...\n")
# create a progress bar
pb <- txtProgressBar(min = 0, max = NCOL(features), style = 3)
# update if label j has the smaller entropy
for(j in index_columns)
{
entropy_and_cutoff_j <- find_split_point(y, features[,j,drop=T])
if(entropy_and_cutoff_j$minimal_entropy < overall_minimal_entropy)
{
overall_minimal_entropy <- entropy_and_cutoff_j$minimal_entropy
best_cutoff <- entropy_and_cutoff_j$best_cutoff
best_feature <- j
}
setTxtProgressBar(pb, j)
}
close(pb)
return(list(minimal_entropy = overall_minimal_entropy,
best_cutoff = best_cutoff,
best_feature = best_feature))
}
entropy_given_number_obs <- function(n_k, n_not_k)
{
n = n_k + n_not_k
pi_k = n_k / n
pi_not_k = n_not_k / n
if(n_k == 0 | n_not_k == 0) return(0) # zero impurity
# compute for entropy function I(N) = -sum(pi(k)*log(pi(k)))
entropy_fun <- function(pi) - pi * log(pi)
entropy <- entropy_fun(pi_k) + entropy_fun(pi_not_k)
return(entropy)
}
#' Function to get entropy for a node set
#'
#' @param y set of values in this node
#' @return The entropy of the set
#'
entropy_node <- function(y)
{
n <- length(y)
classes <- unique(y) # unique labels in the set
I_N <- 0 # the entropy of the set
for(k in classes){
n_Nk <- sum(y == k)
# the entropy for this class
e <- mean(y == k) * entropy_given_number_obs(n_Nk, n - n_Nk)
# update I_N
I_N <- I_N + e
}
return(list(entropy = I_N,
number_values = n))
}
#' Calculate entropy based on a cut point
#'
#' @param indicator vector of logical values (see details)
#' @param y set of outcome values
#' @return entropy
#'
#' @details Function that calculates the entropy based on a set of logical values
#' that indicate to which class (of the two groups in the hypothesis space)
#' the observation belongs to
entropy_overall <- function(indicator, y)
{
n <- length(y)
entropy_left <- entropy_node(y[indicator])
entropy_right <- entropy_node(y[!indicator])
overall_entropy <-
entropy_left$number_values / n * entropy_left$entropy +
entropy_right$number_values / n * entropy_right$entropy
return(overall_entropy)
}
stump <- find_best_split(y_train, x_train)
xaringan:::inf_mr()
xaringan:::inf_mr()
xaringan:::inf_mr()
xaringan:::inf_mr()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
rm(list=ls()) # clear all workspace
library(tidyverse)
rm(list=ls()) # clear all workspace
library(tidyverse)
mpg
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y =hwy))
ggplot(data = mpg)
ggplot(data = mpg)
ggplot(data = mpg) +
geom_point(mapping = aes(x = hwy, y = cyl))
ggplot(data = mpg) +
geom_point(mapping = aes(x = class, y = drv))
xaringan:::inf_mr()
remotes::install_github("anthonynorth/rscodeio")
rscodeio::install_theme()
bookdown:::mathquill()
bookdown:::serve_book()
xaringan:::inf_mr()
xaringan:::inf_mr()
xaringan:::inf_mr()
xaringan:::inf_mr()
xaringan:::inf_mr()
knitr::opts_chunk$set(echo = TRUE)
# clear all workspace
rm(list=ls())
## load rchallenge library
install.packages("rchallenge")
library(rchallenge)
## load "german credit" data
data("german", package = "rchallenge")
## take a look at the daa
str(german)
install.packages("mlr3learners.mboost")
remotes::install_github("mlr3learners/mlr3learners.mboost")
library(mlr3learners.mboost)
learner = lrn("surv.rpart")
library(mlr3)
library(mlr3learners)
learner = lrn("surv.rpart")
remotes::install_github("mlr-org/mlr3extralearners")
learner = lrn("mlr3learners.mboost")
library(mlr3learners.mboost)
learner = lrn("surv.rpart")
install.packages("mlr3extralearners")
## update R version
library(devtools)
install_github(‘andreacirilloac/updateR’)
remotes::install_github(‘andreacirilloac/updateR’)
remotes::install_github("andreacirilloac/updateR")
library(updateR)
library(updateR)
require(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
## update R version
library(devtools)
require(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
updateR()
1
updateR()
## update R version
library(devtools)
devtools::install_github("AndreaCirilloAC/updateR")
library(updateR)
updateR(admin_password = '967270Xgy@')
library(updateR)
updateR()
install.packages(as.vector(installing))
install.packages("mlr3extralearners")
remotes::install_github("mlr-org/mlr3extralearners")
install.packages(c("devtools", "mlr3", "mlr3learners", "remotes"))
install.packages("mlr3extralearners")
remotes::install_github("mlr-org/mlr3extralearners")
library(mlr3learners.mboost)
library(mlr3)
library(mlr3extralearners)
learner = lrn("surv.rpart")
library(mlr3learners)
learner = lrn("surv.rpart")
## load mlr3 learners and extra learners
library(mlr3learners)
library(mlr3)
library(mlr3extralearners)
lrn("regr.gbm")
## install the required learner w.r.t id
install_learners("regr.gbm")
lrn("regr.gbm")
install.packages(..., repos = "https://mlr3learners.github.io/mlr3learners.drat")
install.packages(repos = "https://mlr3learners.github.io/mlr3learners.drat")
## install the required learner w.r.t id
install_learners("surv.coxboost	")
learner <- lrn("surv.coxboost")
install_learners("surv.coxboost")
install.packages("CoxBoost")
learner <- lrn("surv.coxboost")
## load mlr3 learners and extra learners, based on the updated version, we should use the mlr3extralearners package
library(mlr3learners)
library(mlr3)
library(mlr3extralearners)
learner <- lrn("surv.coxboost")
## install the required learner w.r.t id
install_learners("surv.coxboost")
install.packages("CoxBoost")
install.packages("~/Downloads/CoxBoost_1.4.tar.gz", repos = NULL, type = "source")
## install the required learner w.r.t id
install_learners("surv.coxboost")
library(CoxBoost)
install.packages("prodlim")
library(prodlim)
install.packages("~/Downloads/CoxBoost_1.4.tar.gz", repos = NULL, type = "source")
library(CoxBoost)
## install the required learner w.r.t id
install_learners("surv.coxboost")
install.packages("CoxBoost")
install.packages("CoxBoost")
library(CoxBoost)
## install the required learner w.r.t id
install_learners("surv.coxboost")
# install.packages("CoxBoost")
learner <- lrn("surv.coxboost")
learner$param_set
learner$param_set$ids()
## load the required packages for the AFT model
install.packages("mlr3proba")
install.packages("mlr3proba")
install.packages("mlr3proba")
install.packages("mlr3proba")
## load mlr3proba library
library(mlr3proba)
## install the learner with mlr3extralearner
install_learners("surv.rpart")
## load mlr3 learners and extra learners, based on the updated version, we should use the mlr3extralearners package
library(mlr3learners)
## install the learner with mlr3extralearners
install_learners("surv.rpart")
## install the required learner w.r.t id
install_learners("surv.coxboost")
## load mlr3 learners and extra learners, based on the updated version, we should use the mlr3extralearners package
library(mlr3learners)
library(mlr3)
library(mlr3extralearners)
# install.packages("prodlim")
library(prodlim)
install.packages("rpart")
install.packages("distr6")
install.packages("survival")
install.packages("survival")
## load  libraries
library(mlr3proba)
library(rpart)
library(distr6)
library(survival)
install.packages("survival")
install.packages("survival")
## load  libraries
library(mlr3proba)
library(rpart)
library(distr6)
library(survival)
## load mlr3 learners and extra learners, based on the updated version, we should use the mlr3extralearners package
library(mlr3learners)
library(mlr3)
library(mlr3extralearners)
# install.packages("prodlim")
library(prodlim)
library(CoxBoost)
## install the learner with mlr3extralearners
install_learners("surv.rpart")
learner_aft <- lrn("surv.rpart")
learner_aft$param_set$ids()
library(mlr3proba)
## get required learners for the aft model
mlr_learners$get("surv.parametric")
## get required learners for the aft model
mlr_learners$get("surv.parametric")
lrn_aft <- lrn(surv.parametric)
lrn_aft <- lrn("surv.parametric")
## tunable parameters with the aft model
lrn_aft$param_set
## load the mlr3proba library
library(mlr3extralearners)
## get required learners for the coxboost  model
mlr_learners$get("surv.coxboost")
lrn_coxboost <- lrn("surv.coxboost")
## tunable parameters with the aft model
lrn_coxboost$param_set
## load the mlr3 extralearners library
library(mlr3extralearners)
## load the mlr3 extralearners library
library(mlr3extralearners)
## get required learners for the mboost model
mlr_learners$get("surv.mboost")
install.packages("mboost")
## load the mlr3 extralearners library and the mboost library
library(mlr3extralearners)
library(mboost)
## get required learners for the mboost model
mlr_learners$get("surv.mboost")
lrn_mboost <- lrn("surv.mboost")
## tunable parameters with the mboost model
lrn_mboost$param_set
## load the mlr3proba library
library(mlr3proba)
## get required learners for the aft model
mlr_learners$get("surv.parametric")
lrn_aft <- lrn("surv.parametric")
## tunable parameters with the aft model
lrn_aft$param_set
## load the mlr3proba library and the coxboost library
library(mlr3extralearners)
library(CoxBoost)
## get required learners for the coxboost model
mlr_learners$get("surv.coxboost")
lrn_coxboost <- lrn("surv.coxboost")
## tunable parameters with the coxboost model
lrn_coxboost$param_set
## tunable parameters with the mboost model
lrn_mboost$param_set$ids
## tunable parameters with the mboost model
lrn_mboost$param_set$ids()
## tunable parameters with the coxboost model
lrn_coxboost$param_set$ids()
## tunable parameters with the aft model
lrn_aft$param_set$ids()
## tunable parameters with the aft model
lrn_aft$param_set
## tunable parameters with the mboost model
lrn_mboost$param_set$ids()
## tunable parameters with the mboost model
lrn_mboost$param_set
## tunable parameters with the coxboost model
lrn_coxboost$param_set
## load mlr3 learners and extra learners, based on the updated version, we should use the mlr3extralearners package
library(mlr3learners)
library(mlr3)
library(mlr3extralearners)
library(CoxBoost)
## load the mlr3proba library and the coxboost library
library(mlr3extralearners)
library(CoxBoost)
## get required learners for the coxboost model
mlr_learners$get("surv.coxboost")
lrn_coxboost <- lrn("surv.coxboost")
## tunable parameters with the coxboost model
lrn_coxboost$param_set
## load the mlr3 extralearners library and the mboost library
library(mlr3extralearners)
library(mboost)
## get required learners for the mboost model
mlr_learners$get("surv.mboost")
lrn_mboost <- lrn("surv.mboost")
## tunable parameters with the mboost model
lrn_mboost$param_set
# clear all workspace
rm(list=ls())
# install the necessary packages
library(mlr3)
library(mlr3proba)
library(mlr3learners)
library(mlr3extralearners)
library(data.table)
library(mlr3viz)
library(mlr3tuning)
library(mlr3pipelines)
library(purrr)
library(mboost)
library(CoxBoost)
library(survival)
# set or change R working directory
setwd("/Users/echo/Desktop/1_WS20/1_T2E_ML/2_Prediction_Challenge/Data-20210107")
wd = dirname(rstudioapi::getActiveDocumentContext()$path)
# read in the raw data
train_data <- readRDS("train_data.Rds")
test_data <- readRDS("test_list_x.Rds")
# check the data
str(train_data)
# create the corresponding tasks regarding the training dataset
tsks_train = imap(
train_data, # competing risks case
~TaskSurv$new(
id = .y,
backend = .x,
time = "time",
event = "status")
)
## load the learner for semi-parametric aft boosting
lrn_semi_aft = lrn("surv.mboost", sigma = 0, family = 'cindex', baselearner = 'bols', mstop = 5)
# train semi-parametric aft boosting learner
semi_aft_list = map(
tsks_train,
~lrn_semi_aft$train(.x)
)
